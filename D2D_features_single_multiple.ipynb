{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "0e-hZTr2mi-a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQMadgQOmVdj",
        "outputId": "451e6f4b-b6ff-4d6f-a990-29e2838d1431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting https://github.com/debbiemarkslab/EVcouplings/archive/develop.zip\n",
            "  Using cached https://github.com/debbiemarkslab/EVcouplings/archive/develop.zip\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools>=18.2 in /usr/local/lib/python3.10/dist-packages (from evcouplings==0.1.2) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from evcouplings==0.1.2) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evcouplings==0.1.2) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from evcouplings==0.1.2) (1.11.4)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from evcouplings==0.1.2) (0.58.1)\n",
            "Requirement already satisfied: ruamel.yaml in /usr/local/lib/python3.10/dist-packages (from evcouplings==0.1.2) (0.18.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from evcouplings==0.1.2) (3.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from evcouplings==0.1.2) (2.31.0)\n",
            "Requirement already satisfied: mmtf-python in /usr/local/lib/python3.10/dist-packages (from evcouplings==0.1.2) (1.1.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from evcouplings==0.1.2) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from evcouplings==0.1.2) (3.14.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from evcouplings==0.1.2) (5.9.5)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.10/dist-packages (from evcouplings==0.1.2) (3.3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from evcouplings==0.1.2) (3.1.4)\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (from evcouplings==0.1.2) (1.83)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from evcouplings==0.1.2) (0.13.1)\n",
            "Requirement already satisfied: billiard in /usr/local/lib/python3.10/dist-packages (from evcouplings==0.1.2) (4.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from evcouplings==0.1.2) (1.2.2)\n",
            "Requirement already satisfied: contourpy>=1 in /usr/local/lib/python3.10/dist-packages (from bokeh->evcouplings==0.1.2) (1.2.1)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.10/dist-packages (from bokeh->evcouplings==0.1.2) (24.0)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh->evcouplings==0.1.2) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from bokeh->evcouplings==0.1.2) (6.0.1)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.10/dist-packages (from bokeh->evcouplings==0.1.2) (6.3.3)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh->evcouplings==0.1.2) (2024.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->evcouplings==0.1.2) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evcouplings==0.1.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evcouplings==0.1.2) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evcouplings==0.1.2) (2024.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->evcouplings==0.1.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->evcouplings==0.1.2) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->evcouplings==0.1.2) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->evcouplings==0.1.2) (3.1.2)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from mmtf-python->evcouplings==0.1.2) (1.0.8)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->evcouplings==0.1.2) (0.41.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->evcouplings==0.1.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->evcouplings==0.1.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->evcouplings==0.1.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->evcouplings==0.1.2) (2024.6.2)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel.yaml->evcouplings==0.1.2) (0.2.8)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->evcouplings==0.1.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->evcouplings==0.1.2) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evcouplings==0.1.2) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-f749b85109a8>:40: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
            "  from scipy.ndimage.filters import gaussian_filter\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import shutil\n",
        "import copy\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import gc\n",
        "import requests\n",
        "pd.set_option('display.max_rows', None)\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "!pip install https://github.com/debbiemarkslab/EVcouplings/archive/develop.zip\n",
        "from evcouplings.align import Alignment, map_matrix, read_fasta\n",
        "import seaborn as sns\n",
        "from scipy.stats import wilcoxon\n",
        "from collections import OrderedDict, Counter\n",
        "from csv import DictWriter\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import math\n",
        "import warnings\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, TensorDataset\n",
        "from sklearn.preprocessing import MinMaxScaler , StandardScaler\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
        "from numpy import asarray,savez_compressed\n",
        "from sklearn import metrics\n",
        "!pip install -q SentencePiece transformers\n",
        "!pip install sentencepiece\n",
        "from transformers import AdamW, get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup, T5EncoderModel, T5Tokenizer\n",
        "import torch.nn as nn\n",
        "import matplotlib.cm as cm\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from matplotlib import gridspec\n",
        "from sklearn.neighbors import KernelDensity"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "EQ2a8ZmNmkQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculation_confidence(uniprot, all_mutations, msa_path, tokenizer, model, device, m):\n",
        "  print(uniprot)\n",
        "\n",
        "  dif_dif_in, mutations_in, log_prob_in, =[], [], []\n",
        "\n",
        "  ## Read in a sequence alignment from a fasta file\n",
        "  if os.path.isfile(msa_path + uniprot+ \".a3m\"): # True if file exists\n",
        "    name_msa_file = msa_path + uniprot+ \".a3m\"\n",
        "  else:\n",
        "    print('MSA not found in folder !')\n",
        "\n",
        "  ### MSA of gene\n",
        "  aln_subsection = msa_protocol(name_msa_file)\n",
        "\n",
        "  ### Protrans\n",
        "  # calculate the ProTrans for WT protein\n",
        "  lines_list = []\n",
        "  for line in range(len(aln_subsection)):\n",
        "    temp = aln_subsection.matrix[line, :].tolist()\n",
        "\n",
        "    x = [x.upper() for x in temp]\n",
        "    lines_list.append(x)\n",
        "\n",
        "  str1 = \" \"\n",
        "  lines_string = [str1.join(first_line) for first_line in lines_list]\n",
        "  sequences_WT = [re.sub(r\"[-.]\", \"X\", sequence) for sequence in lines_string]\n",
        "\n",
        "  indices_to_excl = []\n",
        "  seq_pooled = []\n",
        "  if aln_subsection.L <501:\n",
        "      BATCH_FILE_SIZE = 15\n",
        "  else:\n",
        "      BATCH_FILE_SIZE = 1\n",
        "\n",
        "  test_features_WT = []\n",
        "  for count in range(0, math.floor(len(sequences_WT) / BATCH_FILE_SIZE)):\n",
        "      i = sequences_WT[count*BATCH_FILE_SIZE:(count+1)*BATCH_FILE_SIZE][:]\n",
        "      ids = tokenizer.batch_encode_plus(i, add_special_tokens=True, padding='longest')\n",
        "      input_ids = torch.tensor(ids['input_ids']).to(device)\n",
        "      attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        embedding = model(input_ids=input_ids,attention_mask=attention_mask)\n",
        "        embedding = embedding.last_hidden_state.cpu().numpy()\n",
        "        for seq_num in range(len(embedding)):\n",
        "          seq_len = (attention_mask[seq_num] == 1).sum()\n",
        "          seq_emd = embedding[seq_num][:seq_len-1]\n",
        "          test_features_WT.append(seq_emd)\n",
        "      del attention_mask\n",
        "      gc.collect()\n",
        "\n",
        "  arr_WT = np.array(test_features_WT)\n",
        "  seq_temp = torch.tensor(arr_WT)\n",
        "  arr_WT = m(seq_temp) # use when you want to reduce dimensions from 1024 to 20\n",
        "  arr_WT =arr_WT.numpy()\n",
        "\n",
        "  ### Calculate differences of all mutations of gene\n",
        "  for k, mut in all_mutations.iterrows():\n",
        "    # check if multiple mutation\n",
        "    if ':' in str(mut['mutant']):\n",
        "        all_positions = str(mut['mutant']).split(':')\n",
        "        temp_averages_log_prob=0\n",
        "        for each_pos in all_positions:\n",
        "            pos = int(each_pos[1:-1])-1\n",
        "            first_col = arr_WT[:, pos]\n",
        "            gmm = GaussianMixture(n_components=1).fit(first_col)\n",
        "            densities_temp = gmm.score_samples(first_col)\n",
        "            threshold_temp = np.percentile(densities_temp, 1)\n",
        "            temp_averages_log_prob += round(Average(densities_temp), 3) # include the WT in the log-probability calculation\n",
        "        averages_log_prob = temp_averages_log_prob/len(all_positions) # average log_probability over all mutations positions\n",
        "\n",
        "    else:\n",
        "        mut_seq = mut['mut_sequence']# mutated sequence\n",
        "        position = int(mut['mutant'][1:-1]) - 1\n",
        "        columns = range(0, arr_WT.shape[1])\n",
        "\n",
        "        first_col = arr_WT[:, position]\n",
        "        gmm = GaussianMixture(n_components=1).fit(first_col)\n",
        "        densities_temp = gmm.score_samples(first_col)\n",
        "        threshold_temp = np.percentile(densities_temp, 1)\n",
        "        averages_log_prob = round(Average(densities_temp), 3) # include the WT in the log-probability calculation\n",
        "\n",
        "    mutant = uniprot+'_'+mut['mutant']\n",
        "    mutations_in.append(mutant) # append mutation\n",
        "    log_prob_in.append(averages_log_prob) # append log-probability of MSA position\n",
        "\n",
        "  return log_prob_in, mutations_in"
      ],
      "metadata": {
        "id": "9FhqOUmUC9KV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NormalizeData(data):\n",
        "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "\n",
        "def notNaN(num):\n",
        "    return num == num\n",
        "\n",
        "def Average(lst):\n",
        "    return sum(lst) / len(lst)\n",
        "\n",
        "def intersection(lst1, lst2):\n",
        "    return list(set(lst1) & set(lst2))\n",
        "\n",
        "def read_a3m(fileobj, inserts=\"first\"):\n",
        "    \"\"\"\n",
        "    Read an alignment in compressed a3m format and expand\n",
        "    into a2m format.\n",
        "    .. note::\n",
        "        this function is currently not able to keep inserts in all the sequences\n",
        "    ..todo::\n",
        "        implement this\n",
        "    Parameters\n",
        "    ----------\n",
        "    fileobj : file-like object\n",
        "        A3M alignment file\n",
        "    inserts : {\"first\", \"delete\"}\n",
        "        Keep inserts in first sequence, or delete\n",
        "        any insert column and keep only match state\n",
        "        columns.\n",
        "    Returns\n",
        "    -------\n",
        "    OrderedDict\n",
        "        Sequences in alignment (key: ID, value: sequence),\n",
        "        in order they appeared in input file\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        Upon invalid choice of insert strategy\n",
        "    \"\"\"\n",
        "    seqs = OrderedDict()\n",
        "\n",
        "    for i, (seq_id, seq) in enumerate(read_fasta(fileobj)):\n",
        "        # remove any insert gaps that may still be in alignment\n",
        "        # (just to be sure)\n",
        "        seq = seq.replace(\".\", \"\")\n",
        "\n",
        "        if inserts == \"first\":\n",
        "            # define \"spacing\" of uppercase columns in\n",
        "            # final alignment based on target sequence;\n",
        "            # remaining columns will be filled with insert\n",
        "            # gaps in the other sequences\n",
        "            if i == 0:\n",
        "                uppercase_cols = [\n",
        "                    j for (j, c) in enumerate(seq)\n",
        "                    if (c == c.upper() or c == \"-\")\n",
        "                ]\n",
        "                gap_template = np.array([\".\"] * len(seq))\n",
        "                filled_seq = seq\n",
        "            else:\n",
        "                uppercase_chars = [\n",
        "                    c for c in seq if c == c.upper() or c == \"-\"\n",
        "                ]\n",
        "                filled = np.copy(gap_template)\n",
        "                filled[uppercase_cols] = uppercase_chars\n",
        "                filled_seq = \"\".join(filled)\n",
        "\n",
        "        elif inserts == \"delete\":\n",
        "            # remove all lowercase letters and insert gaps .;\n",
        "            # since each sequence must have same number of\n",
        "            # uppercase letters or match gaps -, this gives\n",
        "            # the final sequence in alignment\n",
        "            seq = \"\".join([c for c in seq if c == c.upper() and c != \".\"])\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Invalid option for inserts: {}\".format(inserts)\n",
        "            )\n",
        "\n",
        "        seqs[seq_id] = seq\n",
        "\n",
        "    return seqs\n",
        "\n",
        "\n",
        "def intersection(lst1, lst2):\n",
        "    return list(set(lst1) & set(lst2))\n",
        "\n",
        "def msa_protocol(name_msa_file):\n",
        "  ### name_msa_file: path to msa fime of gene\n",
        "  ### return: alignment of gene\n",
        "\n",
        "  with open(name_msa_file, \"r\") as infile:\n",
        "    #seqs = OrderedDict()\n",
        "    next(infile)\n",
        "\n",
        "    #for i, (seq_id, seq) in enumerate(read_fasta(infile)):\n",
        "    proper_infile = read_a3m(infile, inserts = \"delete\") # convert from a3m to a2m\n",
        "\n",
        "    #aln = Alignment.from_file(proper_infile, format=\"fasta\")\n",
        "    aln = Alignment.from_dict(proper_infile)\n",
        "\n",
        "  # Sequence length and number of sequences\n",
        "  #print(f\"alignment is of length {aln.L} and has {aln.N} sequences\")\n",
        "\n",
        "  # Protocol Hopf\n",
        "  # calculate the percent identity of every sequence in the alignment to the first sequence\n",
        "  ident_perc = aln.identities_to(aln.matrix[0])\n",
        "  ident_perc_list = ident_perc.tolist()\n",
        "\n",
        "  # keep identifiers with > 50 percentage identity and colunns with at least 70% occupancy\n",
        "  index_keep = []\n",
        "  for i, iden in enumerate(ident_perc_list):\n",
        "    if iden > 0.5: # 0.5= sequences with at least 50% of identity to the frst sequence are kept\n",
        "      index_keep.append(i)\n",
        "\n",
        "  #use the \"count\" method of the class  -  Percentage of gaps\n",
        "  maximum1 = aln.count(axis=\"seq\",char=\"-\")#.argmax()\n",
        "\n",
        "  filtered_ind = [i for i in range(len(maximum1)) if maximum1[i] <= 0.3] # 0.3 30% of gaps\n",
        "  sequences_to_keep = intersection(index_keep, filtered_ind) # keep indeces that satisfy both conditions\n",
        "\n",
        "  selection_index = sequences_to_keep\n",
        "  aln_subsection = aln.select(sequences=selection_index)\n",
        "  #print(f\"the new alignment has {aln_subsection.N} sequences\")\n",
        "\n",
        "  # if remaining sequences in MSA < 15 redo the process with less strict filtering\n",
        "  if aln_subsection.N <15:\n",
        "    index_keep = []\n",
        "    for i, iden in enumerate(ident_perc_list):\n",
        "      if iden > 0.27: # 0.3= sequences with at least 10% of identity to the frst sequence are kept\n",
        "        index_keep.append(i)\n",
        "    filtered_ind = [i for i in range(len(maximum1)) if maximum1[i] <= 0.7] # max 60% of gaps\n",
        "    sequences_to_keep = intersection(index_keep, filtered_ind) # keep indeces that satisfy both conditions\n",
        "    selection_index = sequences_to_keep\n",
        "    aln_subsection = aln.select(sequences=selection_index)\n",
        "\n",
        "  if aln_subsection.N <15:\n",
        "    index_keep = []\n",
        "    for i, iden in enumerate(ident_perc_list):\n",
        "      if iden > 0.2: # 0.3= sequences with at least 20% of identity to the frst sequence are kept\n",
        "        index_keep.append(i)\n",
        "    filtered_ind = [i for i in range(len(maximum1)) if maximum1[i] <= 0.7] # max 60% of gaps\n",
        "    sequences_to_keep = intersection(index_keep, filtered_ind) # keep indeces that satisfy both conditions\n",
        "    selection_index = sequences_to_keep\n",
        "    aln_subsection = aln.select(sequences=selection_index)\n",
        "\n",
        "  #print(f\"the new alignment has {aln_subsection.N} sequences\")\n",
        "  return aln_subsection\n",
        "\n",
        "def unique(list1):\n",
        "\n",
        "    # initialize a null list\n",
        "    unique_list = []\n",
        "\n",
        "    # traverse for all elements\n",
        "    for x in list1:\n",
        "        # check if exists in unique_list or not\n",
        "        if x not in unique_list:\n",
        "            unique_list.append(x)\n",
        "\n",
        "    print(f'{len(unique_list)} unique transcripts')\n",
        "    return unique_list"
      ],
      "metadata": {
        "id": "upBJbFHHQZLM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalise_confidence(gene_confidence):\n",
        "\n",
        "  # min max\n",
        "  max_log = max(gene_confidence['Log_prob'].tolist())\n",
        "  min_log = min(gene_confidence['Log_prob'].tolist())\n",
        "  gene_confidence['log_normalized'] = (gene_confidence['Log_prob'] - min_log )/(max_log - min_log)\n",
        "  condition1 = gene_confidence['GMM_features'] >= 0.5 # for the 5 initial genes TP53, PTEN, AR, BRAF and ChEK2: D2D_prediction\n",
        "  condition2 = (gene_confidence['log_normalized'] >= 0.5) & (gene_confidence['GMM_features'] < 0.5)\n",
        "  condition3 = (gene_confidence['log_normalized'] < 0.5) & (gene_confidence['GMM_features'] < 0.5)\n",
        "\n",
        "  # when using only log-GMM\n",
        "  gene_confidence.loc[condition1, 'overall_confidence'] = gene_confidence.loc[condition1, 'log_normalized']   # Set values in 'B' as half of values in 'C' when the condition is met\n",
        "  gene_confidence.loc[condition2, 'overall_confidence'] = abs(1- gene_confidence.loc[condition2, 'log_normalized'] ) # *1.2#Set values in 'B' as half of values in 'D' when the condition is not met\n",
        "  gene_confidence.loc[condition3, 'overall_confidence'] = 1- gene_confidence.loc[condition3, 'log_normalized']*1.3\n",
        "\n",
        "  return gene_confidence"
      ],
      "metadata": {
        "id": "IzRXfmXZlKOj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_uniprot_fasta(identifier): #loads fasta file for a given UniProt identifier\n",
        "    #if '-' in identifier:\n",
        "    #  ident = identifier[:-2]\n",
        "    #  which_isoform = int(identifier[-1]) - 1\n",
        "    #  link = \"http://www.uniprot.org/uniprot/\" + ident + \".fasta?include=yes\"\n",
        "    #else:\n",
        "    link = \"http://www.uniprot.org/uniprot/\" + identifier + \".fasta\"\n",
        "\n",
        "    str_data = requests.get(link).content.decode('utf-8')\n",
        "    fasta = str_data.split('>')\n",
        "    fasta_all=[]\n",
        "    for seq in fasta[1:]:\n",
        "      temp = seq.splitlines()[1:]\n",
        "      temp = ''.join(temp)\n",
        "      fasta_all.append(temp)\n",
        "    return fasta_all[0]\n",
        "\n",
        "def subst_download_new(uniprot, start=1, end=1):\n",
        "        '''\n",
        "        Input: Uniprots ID,  for 19 other AA substitutions from start to end\n",
        "        '''\n",
        "        # Download sequence from uniprot\n",
        "        sequence = load_uniprot_fasta(uniprot)\n",
        "        if end ==1:\n",
        "            end= len(sequence) + 1\n",
        "        df= substitute(uniprot, sequence, start, end)\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "AnCYUoFEoR9t"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def substitute(uniprot, sequence, start, end):\n",
        "        '''\n",
        "        Input: Uniprots ID,  for 19 remaining AA substitutions from start to end\n",
        "        '''\n",
        "        sequence_part = list(sequence[start-1:end-1]) # keep posit_range of sequence - example: 193-280\n",
        "\n",
        "        AA_list = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
        "        mut_sequence, AA_targ, AA_orig, position, mutant = [], [], [], [], []\n",
        "\n",
        "        for i, AA in enumerate(sequence_part):\n",
        "                mut_seq = list(sequence)\n",
        "                remaining_AA = AA_list.copy()\n",
        "                remaining_AA.remove(AA)\n",
        "                for k in remaining_AA:\n",
        "                        mut_seq[start+i-1] = k\n",
        "                        mut_sequence.append(''.join(mut_seq))\n",
        "                        AA_orig.append(AA)\n",
        "                        AA_targ.append(k)\n",
        "                        position.append(start+i)\n",
        "                        mutant.append( AA+str(start+i)+k)\n",
        "        d = {'uniprot id': [uniprot]*len(AA_targ), 'WT_sequence' : sequence, 'mut_sequence':mut_sequence, 'AA_orig': AA_orig, 'position' : position, 'AA_targ' : AA_targ, 'mutant': mutant}\n",
        "        df = pd.DataFrame(data =d)\n",
        "        return df\n",
        "\n",
        "def find_WT(uniprot, fasta_uniprot_canonical_path, fasta_uniprot_isoform_path):\n",
        "  # find uniprot WT sequence of protein\n",
        "  result_dict_canonical = fasta_to_dict(fasta_uniprot_canonical_path)\n",
        "  result_dict_isoform = fasta_to_dict(fasta_uniprot_isoform_path)\n",
        "\n",
        "  try:\n",
        "    if uniprot in result_dict_canonical:\n",
        "      WT_sequence = result_dict_canonical[uniprot]\n",
        "\n",
        "    elif uniprot in result_dict_isoform:\n",
        "      WT_sequence = result_dict_isoform[uniprot]\n",
        "    return WT_sequence\n",
        "\n",
        "  except:\n",
        "    print('uniprot not in fasta')\n",
        "    return None\n",
        "\n",
        "def fasta_to_dict(file_path):\n",
        "    fasta_dict = {}\n",
        "\n",
        "    with open(file_path, 'r') as fasta_file:\n",
        "        current_accession = None\n",
        "        current_sequence = []\n",
        "\n",
        "        for line in fasta_file:\n",
        "            line = line.strip()\n",
        "\n",
        "            if line.startswith('>'):\n",
        "                # If a new accession is found, save the previous one (if any)\n",
        "                if current_accession is not None:\n",
        "                    fasta_dict[current_accession] = ''.join(current_sequence)\n",
        "\n",
        "                # Extract the accession from the header line\n",
        "                current_accession = line.split('|')[1]\n",
        "                current_sequence = []\n",
        "            else:\n",
        "                # Append sequence lines\n",
        "                current_sequence.append(line)\n",
        "\n",
        "        # Save the last entry\n",
        "        if current_accession is not None:\n",
        "            fasta_dict[current_accession] = ''.join(current_sequence)\n",
        "\n",
        "    return fasta_dict\n",
        "\n",
        "def save_fasta_file(uniprot, sequence):\n",
        "    file_name = f\"{uniprot}.fasta\"\n",
        "\n",
        "    with open(file_name, 'w') as fasta_file:\n",
        "        fasta_file.write(f\">{uniprot}\\n{sequence}\\n\")\n",
        "\n",
        "def dict_to_fasta(ordered_dict):\n",
        "    fasta_lines = []\n",
        "\n",
        "    for header, sequence in ordered_dict.items():\n",
        "        # Format each entry as a FASTA record\n",
        "        header=header.split('\\t')[0]\n",
        "        fasta_lines.append(f\">{header}\")\n",
        "        fasta_lines.append(sequence)\n",
        "\n",
        "    # Join the lines to create the final FASTA string\n",
        "    fasta_string = \"\\n\".join(fasta_lines)\n",
        "    return fasta_string\n",
        "\n",
        "def save_fasta_to_file(ordered_dict, filename):\n",
        "    fasta_content = dict_to_fasta(ordered_dict)\n",
        "\n",
        "    with open(filename, 'w') as fasta_file:\n",
        "        fasta_file.write(fasta_content)"
      ],
      "metadata": {
        "id": "Yn1t2-_fnR4M"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"GMM feature and Confidence score\"\"\"\n",
        "def calculation_WT_MUT(uniprot, all_mutations, msa_path, tokenizer, model, device, m):\n",
        "  print(uniprot)\n",
        "\n",
        "  dif_dif_in, mutations_in, log_prob_in, =[], [], []\n",
        "\n",
        "  ## Read in a sequence alignment from a fasta file\n",
        "  if os.path.isfile(msa_path + uniprot+ \".a3m\"): # True if file exists\n",
        "    name_msa_file = msa_path + uniprot+ \".a3m\"\n",
        "  else:\n",
        "    print('MSA not found in folder !')\n",
        "\n",
        "  ### MSA of gene\n",
        "  aln_subsection = msa_protocol(name_msa_file)\n",
        "\n",
        "  ### Protrans\n",
        "  # calculate the ProTrans for WT protein\n",
        "  lines_list = []\n",
        "  for line in range(len(aln_subsection)):\n",
        "    temp = aln_subsection.matrix[line, :].tolist()\n",
        "\n",
        "    x = [x.upper() for x in temp]\n",
        "    lines_list.append(x)\n",
        "\n",
        "  str1 = \" \"\n",
        "  lines_string = [str1.join(first_line) for first_line in lines_list]\n",
        "  sequences_WT = [re.sub(r\"[-.]\", \"X\", sequence) for sequence in lines_string]\n",
        "\n",
        "  indices_to_excl = []\n",
        "  seq_pooled = []\n",
        "  if aln_subsection.L <501:\n",
        "      BATCH_FILE_SIZE = 15\n",
        "  else:\n",
        "      BATCH_FILE_SIZE = 1\n",
        "\n",
        "  test_features_WT = []\n",
        "  for count in range(0, math.floor(len(sequences_WT) / BATCH_FILE_SIZE)):\n",
        "      i = sequences_WT[count*BATCH_FILE_SIZE:(count+1)*BATCH_FILE_SIZE][:]\n",
        "      ids = tokenizer.batch_encode_plus(i, add_special_tokens=True, padding='longest')\n",
        "      input_ids = torch.tensor(ids['input_ids']).to(device)\n",
        "      attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        embedding = model(input_ids=input_ids,attention_mask=attention_mask)\n",
        "        embedding = embedding.last_hidden_state.cpu().numpy()\n",
        "        for seq_num in range(len(embedding)):\n",
        "          seq_len = (attention_mask[seq_num] == 1).sum()\n",
        "          seq_emd = embedding[seq_num][:seq_len-1]\n",
        "          test_features_WT.append(seq_emd)\n",
        "      del attention_mask\n",
        "      gc.collect()\n",
        "\n",
        "  arr_WT = np.array(test_features_WT)\n",
        "  print(arr_WT.shape)\n",
        "\n",
        "  # when transform 1024D to 20D\n",
        "  seq_temp = torch.tensor(arr_WT)\n",
        "  arr_WT = m(seq_temp) # use when you want to reduce dimensions from 1024 to 20\n",
        "  arr_WT =arr_WT.numpy()\n",
        "\n",
        "\n",
        "  columns = range(0, arr_WT.shape[1])\n",
        "  differences_WT= []\n",
        "  for col in columns:\n",
        "      first_col = arr_WT[:, col]\n",
        "      gmm = GaussianMixture(n_components=1).fit(first_col)\n",
        "      densities_temp = gmm.score_samples(first_col)\n",
        "      threshold_temp = np.percentile(densities_temp, 1)\n",
        "      differences_WT.append(densities_temp[0] - threshold_temp)\n",
        "\n",
        "  ### Calculate differences of all mutations of gene\n",
        "  for k, mut in all_mutations.iterrows():\n",
        "\n",
        "    # check if multiple mutation\n",
        "    if ':' in str(mut['mutant']):\n",
        "        all_positions = str(mut['mutant']).split(':')\n",
        "        temp_averages_log_prob=0\n",
        "        for each_pos in all_positions:\n",
        "            pos = int(each_pos[1:-1])-1\n",
        "            first_col = arr_WT[:, pos]\n",
        "            gmm = GaussianMixture(n_components=1).fit(first_col)\n",
        "            densities_temp = gmm.score_samples(first_col)\n",
        "            threshold_temp = np.percentile(densities_temp, 1)\n",
        "            temp_averages_log_prob += round(Average(densities_temp), 3) # include the WT in the log-probability calculation\n",
        "        averages_log_prob = temp_averages_log_prob/len(all_positions) # average log_probability over all mutations positions\n",
        "\n",
        "    else:\n",
        "        first_col = arr_WT[:, int(mut['mutant'][1:-1])- 1]\n",
        "        gmm = GaussianMixture(n_components=1).fit(first_col)\n",
        "        densities_temp = gmm.score_samples(first_col)\n",
        "        threshold_temp = np.percentile(densities_temp, 1)\n",
        "        averages_log_prob = round(Average(densities_temp), 3) # include the WT in the log-probability calculation\n",
        "\n",
        "\n",
        "    differences_MUT = []\n",
        "    diction_test = {} # dictionary containing the difference of log-probabilities of mutation from the lof-prob of WT\n",
        "    mut_seq = mut['mut_sequence']# mutated sequence\n",
        "\n",
        "    #deep copy of WT array\n",
        "    arr_WT_MUT = arr_WT.copy()\n",
        "\n",
        "    # Mutant - threshold\n",
        "    new_str = [str(x) for x in mut_seq]\n",
        "\n",
        "    str1 = \" \"\n",
        "    lines_string = str1.join(new_str)\n",
        "    MUT_sequence = re.sub(r\"[-.]\", \"X\", lines_string)\n",
        "\n",
        "    ids = tokenizer.batch_encode_plus([MUT_sequence], add_special_tokens=True, padding='longest')\n",
        "    input_ids = torch.tensor(ids['input_ids']).to(device)\n",
        "    attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      embedding = model(input_ids=input_ids,attention_mask=attention_mask)\n",
        "      embedding = embedding.last_hidden_state.cpu().numpy()\n",
        "      seq_len = (attention_mask == 1).sum()\n",
        "      seq_emd = embedding[:, :seq_len-1, :]\n",
        "\n",
        "    seq_emd = torch.tensor(seq_emd)\n",
        "    seq_emd = m(seq_emd) # use when you want to reduce dimensions from 1024 to 20\n",
        "    seq_emd =seq_emd.numpy()\n",
        "\n",
        "    arr_WT_MUT[0] = seq_emd[0]\n",
        "    del embedding, ids, MUT_sequence, attention_mask\n",
        "    gc.collect()\n",
        "\n",
        "    columns = range(0, arr_WT_MUT.shape[1])\n",
        "\n",
        "    for col in columns:\n",
        "        first_col = arr_WT_MUT[:, col]\n",
        "        gmm = GaussianMixture(n_components=1).fit(first_col)\n",
        "        densities_temp = gmm.score_samples(first_col)\n",
        "        threshold_temp = np.percentile(densities_temp, 1)\n",
        "        differences_MUT.append(densities_temp[0] - threshold_temp)\n",
        "\n",
        "    mutant = uniprot+'_'+mut['mutant']#AA_orig+str(mut['position'])+AA_targ\n",
        "    mutations_in.append(mutant) # append mutation\n",
        "    log_prob_in.append(averages_log_prob) # append log-probability of MSA position\n",
        "    dif_dif_in.append([differences_WT[i] - differences_MUT[i] for i in range(len(differences_MUT))]) # difference of WT and Mutated sequence log probabiities\n",
        "\n",
        "  return log_prob_in, mutations_in, dif_dif_in"
      ],
      "metadata": {
        "id": "C44yIz5BAcfV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load ProtT5 Pretrained Protein model"
      ],
      "metadata": {
        "id": "YNiM6qtORCZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import Pre-Trained model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\", do_lower_case=False )\n",
        "model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "model = model.eval()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlL8ozqZRFos",
        "outputId": "8da435ed-94ab-4c6c-a2a1-ea263fc22b45"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculate all possible mutations of protein (optional)"
      ],
      "metadata": {
        "id": "BLmmDnHsmdey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of P04062 (GBA1_HUMAN)"
      ],
      "metadata": {
        "id": "eyTS7QqDnxCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uniprot = 'P04062'"
      ],
      "metadata": {
        "id": "Wr-UH721szFr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download from Uniprot API"
      ],
      "metadata": {
        "id": "m5LZtdk2oZx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = 1\n",
        "end = 1 # when I mutate the entire sequence\n",
        "df=subst_download_new(uniprot, start, end)\n",
        "df.to_csv(uniprot+ '_all.csv')"
      ],
      "metadata": {
        "id": "uFCN0BUEmiKa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate GMM features and confidence log_prob"
      ],
      "metadata": {
        "id": "WP_s_LVImlr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "protein_list = [uniprot+'_all.csv']\n",
        "prediction_flag= True\n",
        "msa_path= ''\n",
        "m = nn.MaxPool1d(50) # Max Pooling for reduction of features from 1024 to 50 per AA\n",
        "\n",
        "for mega_file in protein_list:\n",
        "    all_mutations = pd.read_csv('P04062_all.csv', sep=',', nrows=4) # CAUTION - for example only 4 mutations are kept\n",
        "\n",
        "\n",
        "    list_uniprot = all_mutations.iloc[0]['uniprot id']\n",
        "\n",
        "    if prediction_flag == True: # calculate the D2D representations\n",
        "        try:\n",
        "            log_prob_temp, mutations, dif_dif = calculation_WT_MUT(list_uniprot, all_mutations, msa_path, tokenizer, model, device, m)\n",
        "            confidence_df = pd.DataFrame(list(zip(mutations, dif_dif, log_prob_temp)), columns = ['mutation', 'D2D_representations', 'Log_prob'])\n",
        "            confidence_df.to_csv('logWT_MUT_'+uniprot+'_confidenceB.csv')\n",
        "        except: # when designed protein, I have no valid MSA\n",
        "            continue\n",
        "\n",
        "    else:    # only calculate confidence\n",
        "        log_prob_temp, mutations = calculation_confidence(list_uniprot, all_mutations, msa_path, tokenizer, model, device, m)\n",
        "        confidence_df = pd.DataFrame(list(zip(mutations, log_prob_temp)), columns = ['mutation','D2D_representations'])\n",
        "        confidence_df.to_csv(uniprot+'_confidenceB.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhFnmiYb_8Pd",
        "outputId": "4ee25274-7861-4b4e-e9fa-de3480c6ce6e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P04062\n",
            "(367, 536, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cucTgD9JR1bR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}