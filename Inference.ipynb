{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "jTCjN5rAX7Wf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GH7nhNqAX1Tl"
      },
      "outputs": [],
      "source": [
        "!pip install Biopython\n",
        "!pip install -q SentencePiece transformers\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import shutil\n",
        "import copy\n",
        "import numpy as np\n",
        "import pickle\n",
        "import gc\n",
        "import requests\n",
        "from sklearn.metrics import precision_recall_fscore_support, matthews_corrcoef, confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
        "from evcouplings.align import Alignment, map_matrix, read_fasta\n",
        "from scipy.stats import wilcoxon\n",
        "from collections import OrderedDict, Counter\n",
        "from csv import DictWriter\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, TensorDataset\n",
        "from sklearn.preprocessing import MinMaxScaler , StandardScaler\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
        "from numpy import asarray,savez_compressed\n",
        "from sklearn import metrics\n",
        "from transformers import AdamW, get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup, T5EncoderModel, T5Tokenizer\n",
        "import torch.nn as nn\n",
        "import matplotlib.cm as cm\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from matplotlib import gridspec\n",
        "from sklearn.neighbors import KernelDensity\n",
        "import random\n",
        "import csv\n",
        "import seaborn as sns\n",
        "from scipy.stats import wilcoxon\n",
        "import plotly.express as px\n",
        "from matplotlib_venn import venn2, venn2_circles\n",
        "import math\n",
        "import imblearn\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import warnings\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "EpMYyPwyX-cw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def NormalizeData(data):\n",
        "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "\n",
        "def notNaN(num):\n",
        "    return num == num\n",
        "\n",
        "def Average(lst):\n",
        "    return sum(lst) / len(lst)\n",
        "\n",
        "def intersection(lst1, lst2):\n",
        "    return list(set(lst1) & set(lst2))\n",
        "\n",
        "def read_a3m(fileobj, inserts=\"first\"):\n",
        "    \"\"\"\n",
        "    Read an alignment in compressed a3m format and expand\n",
        "    into a2m format.\n",
        "    .. note::\n",
        "        this function is currently not able to keep inserts in all the sequences\n",
        "    ..todo::\n",
        "        implement this\n",
        "    Parameters\n",
        "    ----------\n",
        "    fileobj : file-like object\n",
        "        A3M alignment file\n",
        "    inserts : {\"first\", \"delete\"}\n",
        "        Keep inserts in first sequence, or delete\n",
        "        any insert column and keep only match state\n",
        "        columns.\n",
        "    Returns\n",
        "    -------\n",
        "    OrderedDict\n",
        "        Sequences in alignment (key: ID, value: sequence),\n",
        "        in order they appeared in input file\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        Upon invalid choice of insert strategy\n",
        "    \"\"\"\n",
        "    seqs = OrderedDict()\n",
        "\n",
        "    for i, (seq_id, seq) in enumerate(read_fasta(fileobj)):\n",
        "        # remove any insert gaps that may still be in alignment\n",
        "        # (just to be sure)\n",
        "        seq = seq.replace(\".\", \"\")\n",
        "\n",
        "        if inserts == \"first\":\n",
        "            # define \"spacing\" of uppercase columns in\n",
        "            # final alignment based on target sequence;\n",
        "            # remaining columns will be filled with insert\n",
        "            # gaps in the other sequences\n",
        "            if i == 0:\n",
        "                uppercase_cols = [\n",
        "                    j for (j, c) in enumerate(seq)\n",
        "                    if (c == c.upper() or c == \"-\")\n",
        "                ]\n",
        "                gap_template = np.array([\".\"] * len(seq))\n",
        "                filled_seq = seq\n",
        "            else:\n",
        "                uppercase_chars = [\n",
        "                    c for c in seq if c == c.upper() or c == \"-\"\n",
        "                ]\n",
        "                filled = np.copy(gap_template)\n",
        "                filled[uppercase_cols] = uppercase_chars\n",
        "                filled_seq = \"\".join(filled)\n",
        "\n",
        "        elif inserts == \"delete\":\n",
        "            # remove all lowercase letters and insert gaps .;\n",
        "            # since each sequence must have same number of\n",
        "            # uppercase letters or match gaps -, this gives\n",
        "            # the final sequence in alignment\n",
        "            seq = \"\".join([c for c in seq if c == c.upper() and c != \".\"])\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Invalid option for inserts: {}\".format(inserts)\n",
        "            )\n",
        "\n",
        "        seqs[seq_id] = seq\n",
        "\n",
        "    return seqs\n",
        "\n",
        "\n",
        "def intersection(lst1, lst2):\n",
        "    return list(set(lst1) & set(lst2))\n",
        "\n",
        "def msa_protocol(name_msa_file):\n",
        "  ### name_msa_file: path to msa fime of gene\n",
        "  ### return: alignment of gene\n",
        "\n",
        "  with open(name_msa_file, \"r\") as infile:\n",
        "    #seqs = OrderedDict()\n",
        "    next(infile)\n",
        "\n",
        "    #for i, (seq_id, seq) in enumerate(read_fasta(infile)):\n",
        "    proper_infile = read_a3m(infile, inserts = \"delete\") # convert from a3m to a2m\n",
        "    #seq = seq.replace(\".\", \"\")\n",
        "    # seqs[seq_id] = seq\n",
        "    #n_items = take(n, seqs.items())\n",
        "\n",
        "    #aln = Alignment.from_file(proper_infile, format=\"fasta\")\n",
        "    aln = Alignment.from_dict(proper_infile)\n",
        "\n",
        "  # Sequence length and number of sequences\n",
        "  #print(f\"alignment is of length {aln.L} and has {aln.N} sequences\")\n",
        "\n",
        "\n",
        "  # Protocol Hopf\n",
        "  # calculate the percent identity of every sequence in the alignment to the first sequence\n",
        "  ident_perc = aln.identities_to(aln.matrix[0])\n",
        "  ident_perc_list = ident_perc.tolist()\n",
        "\n",
        "  # keep identifiers with > 50 percentage identity and colunns with at least 70% occupancy\n",
        "  index_keep = []\n",
        "  for i, iden in enumerate(ident_perc_list):\n",
        "    if iden > 0.5: # 0.5= sequences with at least 50% of identity to the frst sequence are kept\n",
        "      index_keep.append(i)\n",
        "\n",
        "  #use the \"count\" method of the class  -  Percentage of gaps\n",
        "  maximum1 = aln.count(axis=\"seq\",char=\"-\")#.argmax()\n",
        "\n",
        "  filtered_ind = [i for i in range(len(maximum1)) if maximum1[i] <= 0.3] # 0.3 30% of gaps\n",
        "  sequences_to_keep = intersection(index_keep, filtered_ind) # keep indeces that satisfy both conditions\n",
        "\n",
        "  selection_index = sequences_to_keep\n",
        "  aln_subsection = aln.select(sequences=selection_index)\n",
        "  #print(f\"the new alignment has {aln_subsection.N} sequences\")\n",
        "\n",
        "  # if remaining sequences in MSA < 15 redo the process with less strict filtering\n",
        "  if aln_subsection.N <15:\n",
        "    index_keep = []\n",
        "    for i, iden in enumerate(ident_perc_list):\n",
        "      #if iden > 0.05: # 0.3= sequences with at least 5% of identity to the frst sequence are kept\n",
        "      if iden > 0.27: # 0.3= sequences with at least 10% of identity to the frst sequence are kept\n",
        "        index_keep.append(i)\n",
        "    filtered_ind = [i for i in range(len(maximum1)) if maximum1[i] <= 0.7] # max 60% of gaps\n",
        "    sequences_to_keep = intersection(index_keep, filtered_ind) # keep indeces that satisfy both conditions\n",
        "    selection_index = sequences_to_keep\n",
        "    aln_subsection = aln.select(sequences=selection_index)\n",
        "\n",
        "  if aln_subsection.N <15:\n",
        "    index_keep = []\n",
        "    for i, iden in enumerate(ident_perc_list):\n",
        "      #if iden > 0.05: # 0.3= sequences with at least 5% of identity to the frst sequence are kept\n",
        "      if iden > 0.2: # 0.3= sequences with at least 20% of identity to the frst sequence are kept\n",
        "        index_keep.append(i)\n",
        "    filtered_ind = [i for i in range(len(maximum1)) if maximum1[i] <= 0.7] # max 60% of gaps\n",
        "    sequences_to_keep = intersection(index_keep, filtered_ind) # keep indeces that satisfy both conditions\n",
        "    selection_index = sequences_to_keep\n",
        "    aln_subsection = aln.select(sequences=selection_index)\n",
        "\n",
        "  #print(f\"the new alignment has {aln_subsection.N} sequences\")\n",
        "  return aln_subsection\n",
        "\n",
        "def unique(list1):\n",
        "\n",
        "    # initialize a null list\n",
        "    unique_list = []\n",
        "\n",
        "    # traverse for all elements\n",
        "    for x in list1:\n",
        "        # check if exists in unique_list or not\n",
        "        if x not in unique_list:\n",
        "            unique_list.append(x)\n",
        "\n",
        "    print(f'{len(unique_list)} unique transcripts')\n",
        "    return unique_list\n",
        "\n",
        "\n",
        "def normalise_confidence(gene_confidence):\n",
        "  # min max\n",
        "  max_log = max(gene_confidence['Log_prob'].tolist())\n",
        "  min_log = min(gene_confidence['Log_prob'].tolist())\n",
        "  gene_confidence['log_normalized'] = (gene_confidence['Log_prob'] - min_log )/(max_log - min_log)\n",
        "  condition1 = gene_confidence['D2Deep_prediction'] >= 0.5 # for the 5 initial genes TP53, PTEN, AR, BRAF and ChEK2: D2D_prediction\n",
        "  condition2 = (gene_confidence['log_normalized'] >= 0.5) & (gene_confidence['D2Deep_prediction'] < 0.5)\n",
        "  condition3 = (gene_confidence['log_normalized'] < 0.5) & (gene_confidence['D2Deep_prediction'] < 0.5)\n",
        "\n",
        "  # when using only log-GMM\n",
        "  gene_confidence.loc[condition1, 'overall_confidence'] = gene_confidence.loc[condition1, 'log_normalized']   # Set values in 'B' as half of values in 'C' when the condition is met\n",
        "  gene_confidence.loc[condition2, 'overall_confidence'] = abs(1- gene_confidence.loc[condition2, 'log_normalized'] ) # *1.2#Set values in 'B' as half of values in 'D' when the condition is not met\n",
        "  gene_confidence.loc[condition3, 'overall_confidence'] = 1- gene_confidence.loc[condition3, 'log_normalized']*1.3\n",
        "\n",
        "  return gene_confidence\n",
        "\n",
        "class Classifier2L(nn.Module):\n",
        "    def __init__(self, hidden, hidden2, dropout=0):\n",
        "        super(Classifier2L, self).__init__()\n",
        "        self.hidden = hidden\n",
        "        self.hidden2 = hidden2\n",
        "        self.num_feature = 2200\n",
        "        self.dropout = dropout\n",
        "        self.batchnorm1 = nn.BatchNorm1d(self.hidden)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(self.hidden2)\n",
        "\n",
        "        self.layer_1 = nn.Linear(self.num_feature,  self.hidden)\n",
        "        self.layer_2 = nn.Linear( self.hidden, self.hidden2)\n",
        "        self.layer_3 = nn.Linear( self.hidden2, 1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_1(x)\n",
        "        x = self.batchnorm1(x)\n",
        "        x= self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer_2(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.relu(x)\n",
        "        x= self.dropout(x)\n",
        "\n",
        "        x = self.layer_3(x)\n",
        "        #x = self.sigmoid(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def compute_l1_loss(self, w):\n",
        "        return torch.abs(w).sum()\n",
        "\n",
        "    def compute_l2_loss(self, w):\n",
        "        return torch.square(w).sum()\n",
        "\n",
        "class ClassifierDataset(Dataset):\n",
        "\n",
        "    def __init__(self, X_data, y_data):\n",
        "        self.X_data = X_data\n",
        "        self.y_data = y_data\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index], self.y_data[index]\n",
        "\n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)\n",
        "\n",
        "\n",
        "def predict_protein(mutation_features, model, device, protein_name, length =2200):\n",
        "\n",
        "  # pad to length AA\n",
        "  N= length\n",
        "  fl_dif_pad, positions, proteins_temp =[], [], []\n",
        "  for i, mut in mutation_features.iterrows():\n",
        "    mut_temp = mut.mutation.split('_')[1]\n",
        "    proteins_temp.append(mut.mutation.split('_')[0])\n",
        "    positions.append(mut_temp[1: -1])\n",
        "    #a = mut['fl_dif']\n",
        "    a = mut['Log dif']\n",
        "    new_a = a + [0] * (N - len(a))\n",
        "    fl_dif_pad.append(new_a)\n",
        "  mutation_features['fl_dif_pad'] = fl_dif_pad\n",
        "\n",
        "  stacked_flat_drgn =[]\n",
        "  for i, mut in mutation_features.iterrows():\n",
        "    stacked_flat_drgn.append(torch.tensor(mut['fl_dif_pad']))\n",
        "\n",
        "  stacked_drgn = torch.stack(stacked_flat_drgn)\n",
        "  print(stacked_drgn.shape)\n",
        "\n",
        "  labels_drgn = [random.randint(1, 100) for _ in range(len(stacked_drgn))] # needed for cosntruction of loaders\n",
        "  X_drgn, y_drgn = np.array(stacked_drgn), np.array(labels_drgn)\n",
        "\n",
        "  drgn_dataset = ClassifierDataset(torch.from_numpy(X_drgn).float(), torch.from_numpy(y_drgn).long())\n",
        "  drgn_loader = DataLoader(dataset=drgn_dataset, batch_size=1 , drop_last=True)\n",
        "\n",
        "  y_pred_list = []\n",
        "  predictions_drgn= []\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for X_batch, _ in drgn_loader:\n",
        "          X_batch = X_batch.to(device)\n",
        "          y_test_pred = model(X_batch)\n",
        "          predictions_drgn.extend(torch.sigmoid(y_test_pred).cpu().detach().numpy().tolist())\n",
        "\n",
        "  flat_list = []\n",
        "  for sublist in predictions_drgn:\n",
        "      for item in sublist:\n",
        "          flat_list.append(item)\n",
        "\n",
        "  newList = [round(n, 4) for n in flat_list]\n",
        "\n",
        "  return newList\n",
        "\n",
        "\n",
        "\"\"\"Confidence score\"\"\"\n",
        "def calculation_WT_MUT(uniprot, all_mutations, msa_path, tokenizer, model, device, m):\n",
        "  print(uniprot)\n",
        "\n",
        "  dif_dif_in, mutations_in, log_prob_in, =[], [], []\n",
        "\n",
        "  ## Read in a sequence alignment from a fasta file\n",
        "  if os.path.isfile(msa_path + uniprot+ \".a3m\"): # True if file exists\n",
        "    name_msa_file = msa_path + uniprot+ \".a3m\"\n",
        "  else:\n",
        "    print('MSA not found in folder !')\n",
        "\n",
        "  ### MSA of gene\n",
        "  aln_subsection = msa_protocol(name_msa_file)\n",
        "\n",
        "  ### Protrans\n",
        "  # calculate the ProTrans for WT protein\n",
        "  lines_list = []\n",
        "  for line in range(len(aln_subsection)):\n",
        "    temp = aln_subsection.matrix[line, :].tolist()\n",
        "\n",
        "    x = [x.upper() for x in temp]\n",
        "    lines_list.append(x)\n",
        "\n",
        "  str1 = \" \"\n",
        "  lines_string = [str1.join(first_line) for first_line in lines_list]\n",
        "  sequences_WT = [re.sub(r\"[-.]\", \"X\", sequence) for sequence in lines_string]\n",
        "\n",
        "  indices_to_excl = []\n",
        "  seq_pooled = []\n",
        "  if aln_subsection.L <501:\n",
        "      BATCH_FILE_SIZE = 15\n",
        "  else:\n",
        "      BATCH_FILE_SIZE = 1\n",
        "\n",
        "  test_features_WT = []\n",
        "  for count in range(0, math.floor(len(sequences_WT) / BATCH_FILE_SIZE)):\n",
        "      i = sequences_WT[count*BATCH_FILE_SIZE:(count+1)*BATCH_FILE_SIZE][:]\n",
        "      ids = tokenizer.batch_encode_plus(i, add_special_tokens=True, padding='longest')\n",
        "      input_ids = torch.tensor(ids['input_ids']).to(device)\n",
        "      attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        embedding = model(input_ids=input_ids,attention_mask=attention_mask)\n",
        "        embedding = embedding.last_hidden_state.cpu().numpy()\n",
        "        for seq_num in range(len(embedding)):\n",
        "          seq_len = (attention_mask[seq_num] == 1).sum()\n",
        "          seq_emd = embedding[seq_num][:seq_len-1]\n",
        "          test_features_WT.append(seq_emd)\n",
        "      del attention_mask\n",
        "      gc.collect()\n",
        "\n",
        "  arr_WT = np.array(test_features_WT)\n",
        "  seq_temp = torch.tensor(arr_WT)\n",
        "  arr_WT = m(seq_temp) # use when you want to reduce dimensions from 1024 to 20\n",
        "  arr_WT =arr_WT.numpy()\n",
        "\n",
        "\n",
        "  columns = range(0, arr_WT.shape[1])\n",
        "  differences_WT= []\n",
        "  for col in columns:\n",
        "      first_col = arr_WT[:, col]\n",
        "      gmm = GaussianMixture(n_components=1).fit(first_col)\n",
        "      densities_temp = gmm.score_samples(first_col)\n",
        "      threshold_temp = np.percentile(densities_temp, 1)\n",
        "      differences_WT.append(densities_temp[0] - threshold_temp)\n",
        "\n",
        "\n",
        "  ### Calculate differences of all mutations of gene\n",
        "  for k, mut in all_mutations.iterrows():\n",
        "    differences_MUT = []\n",
        "    AA_orig = mut['AA_orig'] # For ProteinGym\n",
        "    AA_targ = mut['AA_targ'] # For ProteinGym\n",
        "\n",
        "    diction_test = {} # dictionary containing the difference of log-probabilities of mutation from the lof-prob of WT\n",
        "    mut_seq = mut['mut_sequence']# mutated sequence\n",
        "    position = int(mut['position']) - 1\n",
        "    columns = range(0, arr_WT.shape[1])\n",
        "\n",
        "\n",
        "    for col in columns:\n",
        "      if col == position:\n",
        "        first_col = arr_WT[:, col]\n",
        "        gmm = GaussianMixture(n_components=1).fit(first_col)\n",
        "        densities_temp = gmm.score_samples(first_col)\n",
        "        threshold_temp = np.percentile(densities_temp, 1)\n",
        "        averages_log_prob = round(Average(densities_temp), 3) # include the WT in the log-probability calculation\n",
        "        break\n",
        "\n",
        "    #deep copy of WT array\n",
        "    arr_WT_MUT = arr_WT.copy()\n",
        "\n",
        "    # Mutant - threshold\n",
        "    new_str = [str(x) for x in mut_seq]\n",
        "\n",
        "    str1 = \" \"\n",
        "    lines_string = str1.join(new_str)\n",
        "    MUT_sequence = re.sub(r\"[-.]\", \"X\", lines_string)\n",
        "\n",
        "    ids = tokenizer.batch_encode_plus([MUT_sequence], add_special_tokens=True, padding='longest')\n",
        "    input_ids = torch.tensor(ids['input_ids']).to(device)\n",
        "    attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      embedding = model(input_ids=input_ids,attention_mask=attention_mask)\n",
        "      embedding = embedding.last_hidden_state.cpu().numpy()\n",
        "      seq_len = (attention_mask == 1).sum()\n",
        "      seq_emd = embedding[:, :seq_len-1, :]\n",
        "\n",
        "    seq_emd = torch.tensor(seq_emd)\n",
        "    seq_emd = m(seq_emd) # use when you want to reduce dimensions from 1024 to 20\n",
        "    seq_emd =seq_emd.numpy()\n",
        "\n",
        "    arr_WT_MUT[0] = seq_emd[0]\n",
        "    del embedding, ids, MUT_sequence, attention_mask\n",
        "    gc.collect()\n",
        "\n",
        "    columns = range(0, arr_WT_MUT.shape[1])\n",
        "\n",
        "    for col in columns:\n",
        "        first_col = arr_WT_MUT[:, col]\n",
        "        gmm = GaussianMixture(n_components=1).fit(first_col)\n",
        "        densities_temp = gmm.score_samples(first_col)\n",
        "        threshold_temp = np.percentile(densities_temp, 1)\n",
        "        differences_MUT.append(densities_temp[0] - threshold_temp)\n",
        "\n",
        "    mutant = uniprot+'_'+AA_orig+str(mut['position'])+AA_targ\n",
        "    mutations_in.append(mutant) # append mutation\n",
        "    log_prob_in.append(averages_log_prob) # append log-probability of MSA position\n",
        "    dif_dif_in.append([differences_WT[i] - differences_MUT[i] for i in range(len(differences_MUT))]) # difference of WT and Mutated sequence log probabiities\n",
        "\n",
        "  return log_prob_in, mutations_in, dif_dif_in\n",
        "\n",
        "\n",
        "def load_uniprot_fasta(identifier): #loads fasta file for a given UniProt identifier\n",
        "    link = \"http://www.uniprot.org/uniprot/\" + identifier + \".fasta\"\n",
        "\n",
        "    str_data = requests.get(link).content.decode('utf-8')\n",
        "    fasta = str_data.split('>')\n",
        "    fasta_all=[]\n",
        "    for seq in fasta[1:]:\n",
        "      temp = seq.splitlines()[1:]\n",
        "      temp = ''.join(temp)\n",
        "      fasta_all.append(temp)\n",
        "    return fasta_all[0]\n",
        "\n",
        "def subst_download_new(uniprot, start, end):\n",
        "        '''\n",
        "        Input: Uniprots ID,  for 19 other AA substitutions from start to end\n",
        "        '''\n",
        "        # Download sequence from uniprot\n",
        "        sequence = load_uniprot_fasta(uniprot)\n",
        "        df= substitute(sequence, start, end)\n",
        "\n",
        "        return df\n",
        "def substitute(sequence, start, end):\n",
        "        '''\n",
        "        Input: Uniprots ID,  for 19 remaining AA substitutions from start to end\n",
        "        '''\n",
        "        # if entire sequence, uncomment following 2 lines:\n",
        "        # start = 1\n",
        "        # end=len(sequence) +1\n",
        "\n",
        "        sequence_part = list(sequence[start-1:end-1]) # keep posit_range of sequence - example: 193-280\n",
        "\n",
        "        AA_list = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
        "        mut_sequence, AA_targ, AA_orig, position = [], [], [], []\n",
        "\n",
        "\n",
        "        for i, AA in enumerate(sequence_part):\n",
        "                mut_seq = list(sequence)\n",
        "                remaining_AA = AA_list.copy()\n",
        "                remaining_AA.remove(AA)\n",
        "                for k in remaining_AA:\n",
        "                        mut_seq[start+i-1] = k\n",
        "                        mut_sequence.append(''.join(mut_seq))\n",
        "                        AA_orig.append(AA)\n",
        "                        AA_targ.append(k)\n",
        "                        position.append(start+i)\n",
        "        d = {'uniprot id': [uniprot]*len(AA_targ), 'WT_sequence' : sequence, 'mut_sequence':mut_sequence, 'AA_orig': AA_orig, 'position' : position, 'AA_targ' : AA_targ}\n",
        "        df = pd.DataFrame(data =d)\n",
        "        return df\n",
        "\n",
        "def find_WT(uniprot, fasta_uniprot_canonical_path, fasta_uniprot_isoform_path):\n",
        "  # find uniprot WT sequence of protein\n",
        "  result_dict_canonical = fasta_to_dict(fasta_uniprot_canonical_path)\n",
        "  result_dict_isoform = fasta_to_dict(fasta_uniprot_isoform_path)\n",
        "  #print(result_dict_canonical[uniprot])\n",
        "\n",
        "  try:\n",
        "    if uniprot in result_dict_canonical:\n",
        "      WT_sequence = result_dict_canonical[uniprot]\n",
        "\n",
        "    elif uniprot in result_dict_isoform:\n",
        "      WT_sequence = result_dict_isoform[uniprot]\n",
        "    return WT_sequence\n",
        "\n",
        "  except:\n",
        "    print('uniprot not in fasta')\n",
        "    return None\n",
        "\n",
        "def fasta_to_dict(file_path):\n",
        "    fasta_dict = {}\n",
        "\n",
        "    with open(file_path, 'r') as fasta_file:\n",
        "        current_accession = None\n",
        "        current_sequence = []\n",
        "\n",
        "        for line in fasta_file:\n",
        "            line = line.strip()\n",
        "\n",
        "            if line.startswith('>'):\n",
        "                # If a new accession is found, save the previous one (if any)\n",
        "                if current_accession is not None:\n",
        "                    fasta_dict[current_accession] = ''.join(current_sequence)\n",
        "\n",
        "                # Extract the accession from the header line\n",
        "                current_accession = line.split('|')[1]\n",
        "                current_sequence = []\n",
        "            else:\n",
        "                # Append sequence lines\n",
        "                current_sequence.append(line)\n",
        "\n",
        "        # Save the last entry\n",
        "        if current_accession is not None:\n",
        "            fasta_dict[current_accession] = ''.join(current_sequence)\n",
        "\n",
        "    return fasta_dict\n",
        "\n",
        "def save_fasta_file(uniprot, sequence):\n",
        "    file_name = f\"{uniprot}.fasta\"\n",
        "\n",
        "    with open(file_name, 'w') as fasta_file:\n",
        "        fasta_file.write(f\">{uniprot}\\n{sequence}\\n\")\n",
        "\n",
        "def dict_to_fasta(ordered_dict):\n",
        "    fasta_lines = []\n",
        "\n",
        "    for header, sequence in ordered_dict.items():\n",
        "        # Format each entry as a FASTA record\n",
        "        header=header.split('\\t')[0]\n",
        "        fasta_lines.append(f\">{header}\")\n",
        "        fasta_lines.append(sequence)\n",
        "\n",
        "    # Join the lines to create the final FASTA string\n",
        "    fasta_string = \"\\n\".join(fasta_lines)\n",
        "    return fasta_string\n",
        "\n",
        "def save_fasta_to_file(ordered_dict, filename):\n",
        "    fasta_content = dict_to_fasta(ordered_dict)\n",
        "\n",
        "    with open(filename, 'w') as fasta_file:\n",
        "        fasta_file.write(fasta_content)"
      ],
      "metadata": {
        "id": "KJo4M0mxLbTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Pre-Trained model"
      ],
      "metadata": {
        "id": "b7d7XC0iLgaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_pretrained = 'PATH_TO_PRETRAINED_MODEL'"
      ],
      "metadata": {
        "id": "wxGuh3KcPEP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(path_pretrained, do_lower_case=False )\n",
        "model = T5EncoderModel.from_pretrained(path_pretrained)\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "model = model.eval()"
      ],
      "metadata": {
        "id": "oLyrDenzLigs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load D2Deep model"
      ],
      "metadata": {
        "id": "0zyPpl6aGcTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_D2Deep = 'PATH_TO_D2DEEP_MODEL'"
      ],
      "metadata": {
        "id": "Np8wx6mmPUXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = 4096\n",
        "hidden2=2048\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = Classifier2L(h, hidden2, 0.3).to(device)\n",
        "model.load_state_dict(torch.load(path_D2Deep))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "TjG34yZpGdsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference example"
      ],
      "metadata": {
        "id": "88kaWfGdLcil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = nn.MaxPool1d(50) # Max Pooling for reduction of features from 1024 to 50 per AA\n",
        "curwd = os.getcwd()\n",
        "msa_path= '' # path to all msas\n",
        "\n",
        "protein_list = ['Q9BZD2'] # example\n",
        "\n",
        "for uniprot in protein_list:\n",
        "    all_mutations = pd.read_csv(uniprot+'_all.csv')\n",
        "\n",
        "    #Calculate GMM features and confidence log_prob\n",
        "    log_prob_temp, mutations, dif_dif = calculation_WT_MUT(uniprot, all_mutations, msa_path, tokenizer, model, device, m)\n",
        "\n",
        "    confidence_df = pd.DataFrame(list(zip(mutations, dif_dif, log_prob_temp)), columns = ['mutation', 'Log dif', 'Log_prob'])\n",
        "    diction_test = confidence_df.to_dict()\n",
        "\n",
        "    # D2Deep predictions\n",
        "    predictions = predict_protein(confidence_df, D2Deep_model, device, uniprot)\n",
        "    confidence_df['D2Deep_prediction'] = predictions\n",
        "\n",
        "    # final confidence calculation and AF2 addition\n",
        "    confidence_df['uniprot id'] = confidence_df.mutation.str.split(pat='_',expand=True)[0]\n",
        "    confidence_df['conc_mutation'] = confidence_df.mutation.str.split(pat='_',expand=True)[1]\n",
        "    confidence_df['AF2_name'] = ['AF-'+uniprot+'-F1-model_v4'] * len(confidence_df)\n",
        "\n",
        "    final_df = normalise_confidence(confidence_df)\n",
        "    final_df.to_csv(uniprot+'_d2d_results_confidence.csv')"
      ],
      "metadata": {
        "id": "uBdqi_0YLeK1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}